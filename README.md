# Data Engineering Project

## Overview

This project demonstrates my ability to build a data pipeline that fetches real-time grid frequency data, processes it to compute the battery charge/discharge power needed to maintain grid stability, and saves the results in a CSV file. The job is automated and runs daily, with logs tracking progress and errors while unit tests ensure code reliability.

## Components

1. **API Data Fetching**: Retrieves grid frequency data from the Elexon API.
2. **Data Processing**: Converts the frequency data into power metrics for grid stability.
3. **Automated Job**: Uses Python’s `schedule` library to automate the job, running daily.
4. **CSV Export**: Saves the processed results in a CSV file for data analysts.
5. **Testing**: Includes unit tests for each functionality to ensure system reliability.

## Key Project Features

- **Programming Language**: Python
- **Frameworks and Libraries**:
  - `requests` for API calls.
  - `schedule` for daily job automation.
  - `unittest` for unit testing.
  - Logging for tracking errors and events.
- **Output**: CSV file containing average battery power required for half-hour intervals.

---

## Project Setup

### 1. Virtual Environment Setup

To ensure smooth environment management and dependency isolation, a Python virtual environment is used. Below are the steps to set it up:

#### Create and Activate the Virtual Environment:

```bash
# Step 1: Create a virtual environment
python3 -m venv venv

# Step 2: Activate the virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate
```

### 2. Run the Project

Once the environment is set up, follow these steps to run the project:

#### Install the required packages:
Make sure the requirements.txt file includes all necessary packages:
```bash
requests==2.31.0
schedule==1.1.0
```

Then install them:

```bash
pip3 install -r requirements.txt
```
#### Verify the environment:
Run the following command to check that all packages are installed correctly:
```bash
pip3 freeze
```

#### Running the Job:

```bash
python3 main.py
```
This will fetch the frequency data, process it, and save the results to a CSV file.

#### Running Unit Tests:

```bash
python3 -m unittest test_main.py
```
This will run the test suite to validate the code.

### 3. Project Structure



├── fetch_data.py         # Fetches data from the API

├── process_data.py       # Processes frequency data and calculates power

├── main.py               # Main script to run the job

├── test_main.py          # Unit tests for the project

├── utils.py              # Utility functions, e.g., logging

├── requirements.txt      # Project dependencies

├── average_power.csv     # Example output CSV file

├── task_log.log          # Log file generated by the job

└── README.md             # Documentation


## Improvements for Production

In a production environment, this project can be further enhanced with:

	1.	Scalability: Use Apache Spark or Dask for processing larger datasets.
	2.	Resiliency: Implement retry mechanisms using tools like tenacity for API calls.
	3.	Orchestration: Move from schedule to a more robust orchestrator like Apache Airflow or Prefect for better task management.
	4.	Cloud Integration: Deploy the pipeline to cloud infrastructure (e.g., AWS Lambda, Google Cloud Functions) and use S3 for data storage.
	5.	Monitoring: Integrate monitoring tools such as Prometheus or Datadog for real-time tracking and alerting.

## Conclusion

This project is a demonstration of data engineering principles such as API interaction, data processing, job automation, and unit testing. With further improvements, it can scale to handle real-world, production-level workloads.
